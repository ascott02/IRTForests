---

marp: true
theme: default
paginate: true
math: katex
style: |
  section {
    font-size: 140%;
    width: 1280px;
    height: 720px;
    padding-top: 40px;
    padding-bottom: 40px;
    padding-left: 100px;
    padding-right: 100px;
  }
  ul {
    line-height: 1.2;
  }
  li {
    margin-bottom: 0em;
  }
  pre {
    vertical-align: text-top;
    font-size: 60%;
    line-height: 1.0;
  }
  .columns {
    display: flex;
    gap: 1em;
  }
  .col {
    flex: 1;
  }
  img {
    border-radius: .2em;
  }
footer: ATS &copy; 2025, CC-BY 4.0
---

# IRTForests 

<p style="font-size:80%; margin-top:2.5em;">Andrew T. Scott, Fall 2025</p>
<p style="font-size:70%;"><a href="https://github.com/ascott02/IRTForests">github.com/ascott02/IRTForests</a></p>

---

# Item Response Theory + Random Forests

- Trees become respondents, images become items.
- Response matrix records per-tree correctness on held-out examples.
- Goal: explain RF behavior via IRT ability & difficulty signals and vice versa.

---

# Agenda

- Background: IRT + RF primers
- Pipeline: datasets, embeddings, and response matrices
- Case studies: CIFAR (PCA), CIFAR (MobileNet), MNIST
- Cross-study comparison, 2PL/3PL updates, takeaways, next steps

---

# Item Response Theory (IRT) (Wilson, 2005)

**Why? Because performance != ability â€” but theyâ€™re related.**


- Classical Test Theory (CTT) tells us *how someone did on this test.*

- IRT models *how someone would perform on any set of items that measure the same underlying ability*.

- IRT doesnâ€™t replace CTT, it generalizes it with **portable, interpretable measurements** of capability.

<center>

| CTT | IRT |
|-----------------------|----------------------|
| Measures perf. on specific test | Estimates underlying ability |
| Test = sample of items | Items = samples from a calibrated continuum |
| Precision assumed constant | Precision varies with ability |
| Great for grading | Great for understanding and interpretability |

</center>

> A joint calibration framework where ability and difficulty are inferred together, each defined only in relation to the other. 
> Itâ€™s less like grading individuals and more like synchronizing clocks â€” each calibrated against the ensemble.

---


# Item Response Theory Building Blocks

  <div class="col">

## **Core Terms**

- Ability (Î¸): respondent skill; higher â†’ higher success odds (1PL).
- Difficulty (Î´): item hardness; higher â†’ harder even for strong respondents (1PL).
- Discrimination (ğ‘): slope near Î´ (2PL).
- Guessing (ğ‘): floor for multiple-choice exams (3PL).

  </div>
  <div class="col">

## **Forest Analogy**

- Respondents â†’ decision trees on a shared test set.
- Items â†’ images; responses are binary (tree correct?).
- Response matrix $R_{ij} \in \{0,1\}$ feeds variational IRT.
- Outputs: posteriors over Î¸áµ¢, Î´â±¼, and information curves.

  </div>

---

# Rasch (1PL) Model in One Picture

<div class="columns">
  <div class="col">

$$\Pr(R_{ij}=1 \mid \theta_i, \delta_j) = \frac{1}{1 + e^{- (\theta_i - \delta_j)}}$$

- The probability a respondent gets the item correct, given their ability, and the item's difficulty.
- Single global slope keeps parameters on a shared logit scale.
- Î¸ âˆ’ Î´ = 0 â‡’ 50% success; shifts left/right change odds.
- Fisher information peaks where curves are steepest.
- See <a href="https://ascott02.github.io/irt.html">IRT ICC Visualizer</a> for 2PL, 3PL, 4PL

  </div>
  <div class="col" style="text-align:center;">
  <center>
    <img width="85%" src="figures/irt/rasch_curve.png" style="width:100%; border:1px solid #ccc;" />
    <p style="font-size:85%;">1PL Item Characteristic Curves (ICC)</p>
    </center>
  </div>
</div>

---

# IRT Output

- **Ability histograms** flag low-skill trees worth pruning.
- **Difficulty ladders** highlight mislabeled or ambiguous items.
- **Wright maps** overlay Î¸ and Î´ to expose coverage gaps.
- **Information curves** reveal where ensemble confidence is fragile.
- Together they explain *who* struggles and *why*, beyond RF metrics.


---

# Random Forests â€” Many Noisy Trees, One Stable Voice (Breiman, 2001)

- Train trees on bootstrapped samples with random feature subsets to decorrelate their votes.
- Aggregate those votes by majority (classification) or mean (regression) to cut variance.
- **Margin:** gap between the correct class and the runner-up; **entropy:** dispersion of votes.
- Reading the two together exposes how confidentâ€”or conflictedâ€”the forest is, especially once aligned with Î´.


---

# Random Forest Margins â€” How Confident Is the Crowd?

$$ \text{margin}(x_i) =
P_{\text{correct}}(x_i)
- \max_{j \neq \text{true}} P_j(x_i) $$

The **margin** measures how far ahead the correct class is over its nearest competitor.

- **High margin:** trees vote strongly for the right class â†’ confident.  
- **Low or negative margin:** trees disagree or favor another class â†’ uncertain.  

> Think of it as the *vote gap* in an election â€” the wider the gap, the clearer the win.

---

# Ensemble Entropy â€” How Much Do Trees Disagree?

$$ H(x_i) = - \sum_j P_j(x_i) \log_2 P_j(x_i) $$

The **entropy** measures how dispersed the votes are across classes.

- **Low entropy:** trees nearly unanimous â†’ decisive prediction.  
- **High entropy:** votes spread out â†’ uncertainty or class confusion.  

> Within trees, entropy drives splits (purity).  
> Across trees, entropy reveals disagreement â€” the forestâ€™s collective uncertainty.

---

# GenAI in the Loop Scientific Experimentation

- Recursive prompting (akin to <a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents">context engineering</a>) keeps each iteration scoped.
- Ground every cycle in the `README.md` specâ€”goals, datasets, diagnostics.
- Automate the CLI so runs regenerate figures and tables straight into the deck.
- Commit, push, repeat: <a href="https://github.com/ascott02/IRTForests">github.com/ascott02/IRTForests</a>

> Plastic tubes and pots and pans
> Bits and pieces and the magic from the hand - Oingo Boingo, "Weird Science" 1985

---


# Pipeline Overview

<div class="columns">
  <div class="col">

**Data preparation for three studies**

1. Stratified CIFAR-10 subset: 10k / 2k / 2k splits. Resize 64Ã—64, normalize, PCA â†’ 128-D embeddings.
2. Stratified CIFAR-10 subset: 10k / 2k / 2k splits. Resize 64Ã—64, normalize, MobileNet â†’ 960-D embeddings.
3. MNIST mini: 4k / 800 / 800 digits, normalized 28Ã—28 grayscale. Raw pixels.

**Random forest training**

- RF (2000 trees) trained for every study; metrics and importances saved.
- Response matrices saved: CIFAR `(2000 Ã— 2000)` for PCA & MobileNet, MNIST `(2000 Ã— 800)`.

**IRT analysis**
- 1PL Rasch (SVI, 600 epochs) complete for CIFAR+PCA, CIFAR+MobileNet, and MNIST.
- 2PL (SVI, 800 epochs) complete for CIFAR+PCA, CIFAR+MobileNet, and MNIST.
- 3PL (SVI, 1000 epochs) CIFAR MobileNet only.

  </div>
</div>

---

# Datasets Overview

<center>

| Dataset | Train | Val | Test | Feature Pipeline | Notes |
|---|---|---|---|---|---|
| CIFAR-10 subset | 10,000 | 2,000 | 2,000 | PCA-128 / MobileNet-V3 (960-D) | Shared splits Study I & II |
| MNIST mini | 4,000 | 800 | 800 | 28Ã—28 grayscale â†’ raw pixels (no PCA) | Control for clean handwriting |
</center>

- CIFAR runs differ only by embeddings; labels and splits stay fixed.
- MNIST mirrors the workflow to confirm signals on cleaner data.

---

# Study I: CIFAR-10 + PCA-128 Embeddings

---

# Study I Setup: CIFAR-10 + PCA-128

<div class="columns">
  <div class="col">
    <ul>
  <li>Establish the PCA baseline and capture RF uncertainty signals.</li>
  <li>Use IRT to pinpoint weak trees and hard items that motivate stronger features.</li>
  <li>Fix a stratified CIFAR-10 split (10k / 2k / 2k).</li>
  <li>Train 2000 trees and score them on the shared test set.</li>
  <li>Build a 2000 Ã— 2000 response matrix (mean tree accuracy â‰ˆ 0.18).</li>
  <li>Artifacts: metrics, margins, entropy, IRT outputs.</li>
    </ul>
  </div>

  <div class="col">
  <center>

<img width="85%" src="figures/datasets/study1_cifar_samples.png" style="width:100%; border:1px solid #ccc;" />
<p style="font-size:85%; text-align:center;">Study I sample grid â€” stratified CIFAR-10 slices</p>
    </center>
  </div>
</div>

---

# Study I Performance (PCA-128)

<center>

| Metric | Value |
|---|---|
| Test / Val / OOB acc | 0.468 / 0.470 / 0.442 |
| Per-class range | 0.260 (bird) â†’ 0.635 (ship) |
| Mean tree accuracy | 0.1763 |
| Mean margin / entropy | 0.0058 / 2.1723 |
| Î´ negatively correlates with margin (Pearson) | âˆ’0.815 |
| Î´ positively correlates with entropy (Pearson) | 0.687 |

</center>

- Baseline ensemble still underperforms due to weak PCA features yet preserves Î´ alignment.
- Margins hover near zero (mean â‰ˆ0.006) and entropy stays high (2.17), signalling broad disagreementâ€”prime for IRT.

---

# Study I Confusion Matrix

<div class="columns">
  <div class="col">
    <img src="figures/rf_confusion_matrix.png" style="width:95%; border:1px solid #ccc;" />
  </div>
  <div class="col">

- Off-diagonal spikes (cat vs dog, bird vs airplane, horse vs deer) mirror high-Î´ items.
- Ships and trucks still lead the diagonal (â‰ˆ64% / 56% accuracy), yet well short of a clean blockâ€”further underscoring the curation need.

  </div>
</div>

---

# Study I Diagnostics: Ability Profiles

<div class="columns">
  <div class="col">
  <center>
    <img width="85%" src="figures/ability_vs_accuracy.png" style="width:100%; border:1px solid #ccc;" />
    <p style="font-size:85%; text-align:center;">Ability (Î¸) vs tree accuracy â€” Spearman â‰ˆ 0.99</p>
    </center>
  </div>
  <div class="col">
  <center>
  <img width="84%" src="figures/wright_map.png" style="width:95%; border:1px solid #ccc;" />
  <p style="font-size:85%; text-align:center;">Wright map: Î¸ mean â‰ˆ âˆ’11.0 (Ïƒ â‰ˆ 0.56); Î´ mean â‰ˆ 5.8 with a wide tail</p>
    </center>
  </div>
</div>

- Î¸ ranges from about âˆ’12.8 to âˆ’8.9 (mean â‰ˆ âˆ’11.0 Â± 0.56), so even small shifts separate stronger trees by a few percentage points.
- Î´ centers near 5.8 but stretches from roughly âˆ’11.5 to 13.4, highlighting how ambiguous animal items sit far from the easy tail.

---

# Study I Diagnostics: Î´ vs Error Rate

<div class="columns">
  <div class="col">
    <img src="figures/difficulty_vs_error.png" style="width:95%; border:1px solid #ccc;" />
  </div>
  <div class="col">

- Î´ > 0.4 maps to >80% tree errorâ€”mostly ambiguous animalsâ€”while Î´ < âˆ’0.3 becomes â€œfree points.â€
- Pearson â‰ˆ 0.87, Spearman â‰ˆ 0.86: difficulty doubles as an error heat-map.

  </div>
</div>

---

# Study I Diagnostics: Î´ vs RF Signals

<div class="columns">
  <div class="col">
  <center>
  <img src="figures/difficulty_vs_margin.png" style="width:100%; border:1px solid #ccc;" />
  <p style="font-size:85%;">PCA run: Î´ vs margin (Pearson âˆ’0.82)</p>
    </center>
  </div>
  <div class="col">
  <center>
  <img src="figures/difficulty_vs_entropy.png" style="width:100%; border:1px solid #ccc;" />
  <p style="font-size:85%;">PCA run: Î´ vs entropy (Pearson 0.69)</p>
    </center>
  </div>
</div>

- Hard items cluster bottom-right (low margin, high entropy); opposite corner houses easy wins.
- Study II mirrors the trend with even stronger correlations.

---

# Study I Evidence: Hard vs Easy Examples

<div class="columns">
  <div class="col">

![Hardest items](figures/hardest_items_test.png)

  </div>
  <div class="col">

![Easiest items](figures/easiest_items_test.png)

  </div>
</div>

- Hardest items skew toward ambiguous airplane/ship silhouettes and cluttered cat/dog scenes.
- Easy set is dominated by high-contrast cues (e.g., red fire trucks), yielding low Î´ and entropy.

---


# Study I Fit Checks & Edge Cases

<div class="columns">
  <div class="col">

**Fit diagnostics**

<center>

| Metric | Value |
|---|---|
| Item infit Î¼ / p95 | 0.18 / 0.35 |
| Item outfit Î¼ / p95 | 0.18 / 0.34 |
| Tree infit Î¼ / p95 | 0.35 / 0.48 |
| Tree outfit Î¼ / p95 | 0.18 / 0.19 |
</center>

- MSQs well below 1 show tree responses are steadier than a pure Rasch prior; |z| never exceeds 0.05.

  </div>
  <div class="col">

**Edge cases worth a look**

- `#118` bird â†’ deer votes (Î´ â‰ˆ 13.4, margin â‰ˆ âˆ’0.05, entropy â‰ˆ 2.28).
- `#1734` truck â†’ cat/frog split (Î´ â‰ˆ 13.2, margin â‰ˆ âˆ’0.09, entropy â‰ˆ 2.27).
- `#1602` horse â†’ dog/horse tie (Î´ â‰ˆ 13.2, margin â‰ˆ âˆ’0.11, entropy â‰ˆ 2.22).

- Each item sits below 9% tree accuracyâ€”prime targets for relabeling or curated augmentations.

<center>
    <img width="60%" src="figures/study1_edge_cases.png" style="width:100%; border:1px solid #ccc; margin-top:0.8em;" />
    <p style="font-size:75%; text-align:center;">Study I edge cases Â· IDs 118, 1734, 1602</p>
</center>

  </div>
</div>

---

# Study I Takeaways

- Weak PCA features create long tails in both ability (Î¸) and difficulty (Î´), exposing erratic trees.
- Margin and entropy correlate with Î´, but clusters of high-difficulty animals persist across diagnostics.
- Visual inspection confirms mislabeled or low-signal items driving high Î´, motivating feature upgrades.

---

# Study II: CIFAR-10 + MobileNet Embeddings


---

# Study II Setup: CIFAR-10 + MobileNet-V3

<div class="columns">
  <div class="col">

- Hold the splits fixed to isolate feature gains.
- Swap PCA features for MobileNet-V3 (960-D) while keeping tree count and splits constant.
- Test whether richer embeddings tighten Î¸ spread and retain Î´ alignment.
- Compare RF metrics, uncertainty signals, and IRT parameters against the baseline.

  </div>
  <div class="col">
    <img src="figures/datasets/study2_cifar_samples.png" style="width:100%; border:1px solid #ccc;" />
    <p style="font-size:85%; text-align:center;">Study II sample grid â€” same splits, MobileNet embeddings</p>
  </div>
</div>

---

# Study II Performance (MobileNet-V3)

<center>

| Metric | Value |
|---|---|
| Test / Val / OOB acc | 0.819 / 0.820 / 0.812 |
| Per-class range | 0.695 (bird) â†’ 0.925 (ship) |
| Mean tree accuracy | 0.4792 |
| Mean margin / entropy | 0.2806 / 1.4929 |
| Î´ negatively correlates with margin (Pearson) | âˆ’0.950 |
| Î´ positively correlates with entropy (Pearson) | 0.881 |
</center>

- Pretrained features boost accuracy by 35 pp while strengthening Î´ correlations.
- Higher margins and lower entropy show confidence gains except on stubborn animal classes.
- Artifacts: metrics, response matrix, signals, and IRT outputs under `data/mobilenet/`.

---

# Study II Diagnostics: Î´ vs RF Signals

<div class="columns">
  <div class="col">
  <center>
    <img width="85%" src="figures/mobilenet/mobilenet_difficulty_vs_margin.png" style="width:100%; border:1px solid #ccc;" />
  <p style="font-size:85%;">Î´ vs margin (Pearson âˆ’0.95)</p>
  </center>
  </div>
  <div class="col">
  <center>
    <img width="85%" src="figures/mobilenet/mobilenet_difficulty_vs_entropy.png" style="width:100%; border:1px solid #ccc;" />
  <p style="font-size:85%;">Î´ vs entropy (Pearson 0.88)</p>
  </center>
  </div>
</div>

- MobileNet compresses the easy cluster (high margin, low entropy) while isolating true hard cases.
- Larger |corr| values show tighter agreement between Î´ and RF uncertainty.
- Cat/dog confusions persist, marking curation targets.

---

# Study II Diagnostics: Ability Profiles

<div class="columns">
  <div class="col">
  <center>
    <img width="85%" src="figures/mobilenet/ability_vs_accuracy.png" style="width:100%; border:1px solid #ccc;" />
  <p style="font-size:85%; text-align:center;">Ability (Î¸) vs tree accuracy â€” Pearson 0.96</p>
  </center>
  </div>
  <div class="col">
  <center>
    <img width="84%" src="figures/mobilenet/wright_map.png" style="width:100%; border:1px solid #ccc;" />
  <p style="font-size:85%; text-align:center;">Wright map: Î¸ â‰ˆ âˆ’0.46 Â± 0.23; Î´ spans Â±2.1</p>
  </center>
  </div>
</div>

- Î¸ mean âˆ’0.46 Â± 0.23 keeps the ensemble tightly banded while still ranking trees cleanly.
- Ability remains tied to per-tree accuracy, so feature qualityâ€”rather than tree diversityâ€”now caps gains.

---

# Study II Diagnostics: Î´ vs Error Rate

<div class="columns">
  <div class="col">
    <img src="figures/mobilenet/difficulty_vs_error.png" style="width:95%; border:1px solid #ccc;" />
  </div>
  <div class="col">

- Pearson 0.99 keeps Î´ aligned with mean tree error even at the higher accuracy ceiling.
- Hardest items (Î´ > 1.5) persistâ€”mostly cat/dog overlaps and ambiguous aircraftâ€”while the easy zone (Î´ < âˆ’1) expands.

  </div>
</div>

---

# Study II Evidence: Hard vs Easy Examples

<div class="columns">
  <div class="col">

![MobileNet hardest items](figures/mobilenet/hardest_items_test.png)

  </div>
  <div class="col">

![MobileNet easiest items](figures/mobilenet/easiest_items_test.png)

  </div>
</div>

- MobileNet tightens easy clusters yet the same cat/dog outliers survive with Î´ > 1.5.
- Easy wins sharpen into high-contrast ships and trucks, showing how feature upgrades cleanly separate low-Î´ items.

---

# Study II Fit Checks & Edge Cases

<div class="columns">
  <div class="col">

**Fit diagnostics**

<center>

| Metric | Value |
|---|---|
| Item infit Î¼ / p95 | 0.27 / 0.37 |
| Item outfit Î¼ / p95 | 0.27 / 0.37 |
| Tree infit Î¼ / p95 | 0.29 / 0.31 |
| Tree outfit Î¼ / p95 | 0.27 / 0.29 |
</center>

- Narrow MSQ spread (â‰¤0.37) confirms MobileNet trees behave consistently; no misfit flags at |z| > 0.05.

  </div>
  <div class="col">

**Edge cases worth a look**

- `#1190` automobile â†’ frog votes (Î´ â‰ˆ 15.4, margin â‰ˆ âˆ’0.22, entropy â‰ˆ 1.85; top probs frog 0.28, deer 0.27).
- `#1196` bird â†’ horse (Î´ â‰ˆ 14.9, margin â‰ˆ âˆ’0.38, entropy â‰ˆ 1.31; horse 0.41, deer 0.41, bird 0.08).
- `#95` frog â†’ bird (Î´ â‰ˆ 14.8, margin â‰ˆ âˆ’0.25, entropy â‰ˆ 1.89; bird 0.32, deer 0.20, frog 0.17).

- These persistent outliers survive the feature upgradeâ€”queue them for image/label review next.
<center>
    <img width="60%" src="figures/study2_edge_cases.png" style="width:100%; border:1px solid #ccc; margin-top:0.8em;" />
    <p style="font-size:75%; text-align:center;">Study II edge cases Â· IDs 1190, 1196, 95</p>
</center>

  </div>
</div>

---

# Study II Takeaways

- MobileNet embeddings add 35 pp of accuracy while maintaining a focused ability band (Std(Î¸) â‰ˆ 0.23).
- Î´ stays aligned with RF uncertainty, isolating a smaller yet stubborn ambiguous cluster.
- Residual cat/dog confusion points to data curation as the next lever.

---

# Section III Â· Control Study (MNIST)

---

# Study III Setup: MNIST Mini-Study

<div class="columns">
  <div class="col">

- Probe the pipeline on a high-signal, low-noise dataset.
- Use a lightweight handwriting set to validate RF Ã— IRT beyond CIFAR-10.
- Confirm that IRT still mirrors RF uncertainty when accuracy is near perfect.
- Treat it as a control case where ambiguity is rare yet still detectable.


  </div>
  <div class="col">

  <center>

<img src="figures/datasets/mnist_samples.png" style="width:100%; border:1px solid #ccc;" />
<p style="font-size:85%; text-align:center;">Study III sample grid â€” curated MNIST mini split</p>
    </center>
  </div>
</div>

---

# Study III Performance (MNIST)

<center>

| Metric | Value |
|---|---|
| Train / Val / Test | 4000 / 800 / 800 |
| RF test / val / OOB | 0.954 / 0.944 / 0.939 |
| Mean margin / entropy | 0.5644 / 1.0768 |
| Î´ negatively correlates with margin (Pearson) | âˆ’0.975 |
| Î´ positively correlates with entropy (Pearson) | 0.970 |
| Î¸ mean Â± std | 3.04 Â± 0.29 |
| Î´ mean Â± std | âˆ’0.13 Â± 0.47 |
</center>

- Ambiguous digits (e.g., brushed 5 vs 6) still spike Î´ toward the positive tail; elsewhere the forest is decisive.
- Low entropy + high margin line up with low Î´, giving a â€œsanity benchmarkâ€ beyond CIFAR.

---

# Study III Diagnostics: Î´ vs RF Signals

<div class="columns">
  <div class="col">
    <img src="figures/mnist/mnist_difficulty_vs_margin.png" style="width:100%; border:1px solid #ccc;" />
  <p style="font-size:85%;">Î´ vs margin (Pearson âˆ’0.97)</p>
  </div>
  <div class="col">
    <img src="figures/mnist/mnist_difficulty_vs_entropy.png" style="width:100%; border:1px solid #ccc;" />
  <p style="font-size:85%;">Î´ vs entropy (Pearson 0.97)</p>
  </div>
</div>

- Clean digits show near-perfect alignment between Î´ and RF uncertainty.
- Only a handful of Î´ > 1.2 digits drive the residual uncertainty (stroke collisions like 3/5, 4/9).

---

# Study III Diagnostics: Ability Profiles

<div class="columns">
  <div class="col">
  <center>
    <img width="75%" src="figures/mnist/ability_vs_accuracy.png" style="width:100%; border:1px solid #ccc;" />
  <p style="font-size:85%; text-align:center;">Ability (Î¸) vs tree accuracy â€” Pearson 0.98</p>
  </center>
  </div>
  <div class="col">
  <center>
    <img width="74%" src="figures/mnist/wright_map.png" style="width:100%; border:1px solid #ccc;" />
  <p style="font-size:85%; text-align:center;">Wright map: Î¸ mean 3.04 Â± 0.29; Î´ mean âˆ’0.13 Â± 0.47</p>
  </center>
  </div>
</div>

- Î¸ mean 3.04 Â± 0.29 shows strong consensus, while Î´ mean âˆ’0.13 Â± 0.47 keeps a modest positive tail for ambiguous strokes.
- Shared scales expose plentiful easy wins with only a few sharp spikesâ€”opposite of the CIFAR baseline.


---

# Study III Diagnostics: Î´ vs Error Rate

<div class="columns">
  <div class="col">
    <img src="figures/mnist/difficulty_vs_error.png" style="width:95%; border:1px solid #ccc;" />
  </div>
  <div class="col">

- Pearson 0.98 keeps Î´ tied to mean tree error despite the high accuracy ceiling.
- Î´ > 1.2 corresponds to stroke-collided 3/5/8 and 4/9 pairs; the long negative tail is trivial for the ensemble.

  </div>
</div>

---

# Study III Evidence: Hard vs Easy Digits

<div class="columns">
  <div class="col">

![MNIST hardest digits](figures/mnist/hardest_digits_test.png)

  </div>
  <div class="col">

![MNIST easiest digits](figures/mnist/easiest_digits_test.png)

  </div>
</div>

- Hardest digits show stroke collisions (3 vs 5, 4 vs 9) that push Î´ above 1 despite high margins elsewhere.
- Easy digits are crisp, centered strokesâ€”useful anchors when explaining why Î´ plunges on most of the dataset.

---

# Study III Fit Checks & Edge Cases

<div class="columns">
  <div class="col">

**Fit diagnostics**

<center>

| Metric | Value |
|---|---|
| Item infit Î¼ / p95 | 0.23 / 0.38 |
| Item outfit Î¼ / p95 | 0.22 / 0.37 |
| Tree infit Î¼ / p95 | 0.30 / 0.32 |
| Tree outfit Î¼ / p95 | 0.22 / 0.25 |
</center>

- Rasch residuals stay tight (|z| < 0.07), confirming the control studyâ€™s consistency.

  </div>
  <div class="col">

**Edge cases worth a look**

- `#296` digit 0 â†’ vote 7 (Î´ â‰ˆ 17.6, margin â‰ˆ âˆ’0.35, entropy â‰ˆ 1.83; top probs 7=0.38, 9=0.18, 4=0.16).
- `#151` digit 9 â†’ vote 6 (Î´ â‰ˆ 17.3, margin â‰ˆ âˆ’0.34, entropy â‰ˆ 1.93; top probs 6=0.39, 5=0.12, 2=0.10).
- `#708` digit 4 â†’ vote 3 (Î´ â‰ˆ 16.3, margin â‰ˆ âˆ’0.08, entropy â‰ˆ 2.10; top probs 3=0.19, 4=0.18, 9=0.15).

- Archive these strokes for a â€œconfusing digitsâ€ gallery or curation playbook.

<center>
    <img width="60%" src="figures/study3_edge_cases.png" style="width:100%; border:1px solid #ccc; margin-top:0.8em;" />
    <p style="font-size:75%; text-align:center;">Study III edge cases Â· IDs 296, 151, 708</p>
<center>

  </div>
</div>

---

# Study III Takeaways

- Î´ and RF uncertainty agree almost perfectly, while Î¸ stays high yet still flags the rare ambiguous strokes.
- The control study confirms the RF Ã— IRT pipeline holds outside noisy vision data.

---

# Section IV Â· Cross-Study & Diagnostics

- Compare backbones and datasets on a shared Î¸/Î´ scale.
- Surface recurring themes before the close.

---

# Cross-Study Snapshot

<small>

| Study | Feature Backbone | Test Acc | Î´ negatively correlates with margin (Pearson) | Î´ positively correlates with entropy (Pearson) | Std(Î¸) | Std(Î´) |
|---|---|---|---|---|---|---|
| Study I: CIFAR + PCA-128 | PCA-128 | 0.468 | âˆ’0.815 | 0.687 | 0.154 | 0.150 |
| Study II: CIFAR + MobileNet | MobileNet-V3 (960-D) | 0.819 | âˆ’0.950 | 0.881 | 0.228 | 0.871 |
| Study III: MNIST Mini | Raw pixels | 0.954 | âˆ’0.975 | 0.970 | 0.289 | 0.472 |

</small>

- <small>*Std(Î¸) measures tree ability spread; Std(Î´) measures item difficulty spread.*
- Î´ stays negative with margin and positive with entropy for every study (âˆ’0.82/âˆ’0.95/âˆ’0.98 vs +0.69/+0.88/+0.97).
- Î¸ spread remains compact (Std(Î¸) â‰ˆ 0.15â€“0.29); MobileNet is only slightly wider as headroom grows.
- Difficulty variance jumps on MobileNet (Std(Î´) â‰ˆ 0.87) while MNIST stays moderate, highlighting how rich features surface nuanced â€œhardâ€ digits.
</small>

---

# Cross-Study Fit Snapshot

<center>

| Study | Item infit Î¼ / p95 | Item outfit Î¼ / p95 | Tree infit Î¼ / p95 | Tree outfit Î¼ / p95 |
|---|---|---|---|---|
| CIFAR + PCA | 0.18 / 0.35 | 0.18 / 0.34 | 0.35 / 0.48 | 0.18 / 0.19 |
| CIFAR + MobileNet | 0.27 / 0.37 | 0.27 / 0.37 | 0.29 / 0.31 | 0.27 / 0.29 |
| MNIST mini | 0.23 / 0.38 | 0.22 / 0.37 | 0.30 / 0.32 | 0.22 / 0.25 |

</center>

- All MSQs stay well below 1, indicating over-dispersed errors are rare and Rasch assumptions hold after 2000-tree scaling.
- MobileNetâ€™s slight lift in item MSQ reflects richer feature diversity, while MNIST keeps both item and tree fits exceptionally tight.

---

# 2PL Discrimination (CIFAR + PCA)

- 800-epoch 2PL fit (lr 0.02) yields mean ğ‘ â‰ˆ **0.35 Â± 0.10** (range 0.07â€“0.71).
- ğ‘ correlates with margin at **âˆ’0.83** and with entropy at **+0.63**, aligning slope with RF uncertainty signals.
- Discrimination peaks on the low-margin, high-entropy animal items and steadily tapers for easier scenes, leaving high-margin images with softer slopes.

<div class="columns">
  <div class="col">

  <center>
    <img width="85%" src="figures/2pl_discrimination_vs_margin.png" style="width:100%; border:1px solid #ccc;" />
  </center>
  </div>
  <div class="col">

  <center>
    <img width="85%" src="figures/2pl_discrimination_vs_entropy.png" style="width:100%; border:1px solid #ccc;" />
  </center>
  </div>
</div>

---

# 2PL Discrimination (CIFAR + MobileNet)

- Mean ğ‘ settles at **0.27 Â± 0.15** with a modest tail (max â‰ˆ1.16).
- ğ‘ correlates with margin at **âˆ’0.32** and with entropy at **+0.10**, keeping residual cat/dog confusion in focus while the easy cluster sharpens.
- Discrimination concentrates in the tails: hard animal confusions and trivially easy scenes separate trees, while mid-uncertainty items contribute little.

<div class="columns">
  <div class="col">

  <center>
    <img width="85%" src="figures/mobilenet_2pl_discrimination_vs_margin.png" style="width:100%; border:1px solid #ccc;" />
  </center>
  </div>
  <div class="col">

  <center>
    <img width="85%" src="figures/mobilenet_2pl_discrimination_vs_entropy.png" style="width:100%; border:1px solid #ccc;" />
  </center>
  </div>
</div>


---

# 2PL Discrimination (MNIST)

- Mean ğ‘ lifts to **0.33 Â± 0.25**, so only a modest slice of digits remains truly separating despite the high accuracy ceiling.
- ğ‘ correlates with margin at **+0.89** while its correlation with entropy flips to **âˆ’0.96**â€”uncertainty vanishes outside the awkward strokes.
- Discrimination climbs with margin and falls with entropy: crisp, easy digits carry the steepest slopes while ambiguous stroke collisions stay much flatter.

<div class="columns">
  <div class="col">

  <center>
    <img width="85%" src="figures/mnist_2pl_discrimination_vs_margin.png" style="width:100%; border:1px solid #ccc;" />
  </center>
  </div>
  <div class="col">

  <center>
    <img width="85%" src="figures/mnist_2pl_discrimination_vs_entropy.png" style="width:100%; border:1px solid #ccc;" />
  </center>
  </div>
</div>

---

# 3PL Pilot Â· MobileNet

- 1k-epoch 3PL run (lr 0.01) lands at guess mean **0.35 Â± 0.16**.
- Î¸ vs accuracy stays tight (Pearson **0.98**); slopes average **0.32 Â± 0.08** with a broader tail.
- High guess mass piles onto the ambiguous animal scenes (low margin, high entropy), reinforcing the â€œguessingâ€ narrative.

<div class="columns">
  <div class="col">
    <img src="figures/mobilenet_3pl_guess_hist.png" style="width:100%; border:1px solid #ccc;" />
    <p style="font-size:72%; text-align:center;">3PL MobileNet Â· Guess distribution</p>
  </div>
  <div class="col">
    <img src="figures/mobilenet_3pl_guess_vs_margin.png" style="width:100%; border:1px solid #ccc;" />
    <p style="font-size:72%; text-align:center;">3PL MobileNet Â· Guess vs Margin (colored by entropy)</p>
  </div>
</div>

---

# Tree Attribute Correlations Â· OOB Accuracy vs Î¸

- `scripts/analyze_tree_attribute_correlations.py` merges each treeâ€™s depth/leaves/OOB stats with Î¸ and discrimination aggregates.
- Pearson r (OOB accuracy, Î¸): PCA **+0.25**, MobileNet **+0.70**, MNIST **+0.39** â€” reliable trees earn higher ability across every study.
- CSV/JSON exports: `data/*/tree_attributes_with_signals.csv`, `data/*/tree_attribute_correlations*.json` for deeper dives.

<div class="columns">
  <div class="col">
    <p style="font-size:78%; text-align:center; margin-bottom:0.4em;">PCA Â· OOB accuracy vs Î¸ (r = +0.25)</p>
    <img src="figures/pca_tree_oob_accuracy_vs_theta.png" style="width:100%; border:1px solid #ccc;" />
  </div>
  <div class="col">
    <p style="font-size:78%; text-align:center; margin-bottom:0.4em;">MobileNet Â· OOB acc vs Î¸ (r = +0.70)</p>
    <img src="figures/mobilenet_tree_oob_accuracy_vs_theta.png" style="width:100%; border:1px solid #ccc;" />
  </div>
  <div class="col">
    <p style="font-size:78%; text-align:center; margin-bottom:0.4em;">MNIST Â· OOB accuracy vs Î¸ (r = +0.39)</p>
    <img src="figures/mnist_tree_oob_accuracy_vs_theta.png" style="width:100%; border:1px solid #ccc;" />
  </div>
</div>

---

# Tree Attribute Correlations Â· Leaf Count vs Î¸

- Pearson r (leaf count, Î¸): PCA **âˆ’0.27**, MobileNet **âˆ’0.73**, MNIST **âˆ’0.38** â€” pruning shallower trees boosts ability rankings.
- Leaf count penalizes overfitting branches; MobileNet shows the steepest drop because high-quality features reward compact trees.

<div class="columns">
  <div class="col">
    <p style="font-size:78%; text-align:center; margin-bottom:0.4em;">PCA Â· Leaf count vs Î¸ (r = âˆ’0.27)</p>
    <img src="figures/pca_tree_n_leaves_vs_theta.png" style="width:100%; border:1px solid #ccc;" />
  </div>
  <div class="col">
    <p style="font-size:78%; text-align:center; margin-bottom:0.4em;">MobileNet Â· Leaf count vs Î¸ (r = âˆ’0.73)</p>
    <img src="figures/mobilenet_tree_n_leaves_vs_theta.png" style="width:100%; border:1px solid #ccc;" />
  </div>
  <div class="col">
    <p style="font-size:78%; text-align:center; margin-bottom:0.4em;">MNIST Â· Leaf count vs Î¸ (r = âˆ’0.38)</p>
    <img src="figures/mnist_tree_n_leaves_vs_theta.png" style="width:100%; border:1px solid #ccc;" />
  </div>
</div>


---

# Key Takeaways

- IRT and RF still move in lockstep: Î¸ tracks per-tree accuracy, while Î´ and ğ‘ surface stubborn item pockets.
- MobileNetâ€™s discrimination tail isolates animal confusions despite stronger features; MNIST flips signs because mistakes are rare.
- 3PL adds a modest guessing floor (~0.25) without upsetting Î¸â€“accuracy alignment.
- Tree attributes expose pruning cues: shallow, high-OOB trees consistently land higher Î¸.

---

# Next Steps

- Run stability sweeps (50/100 trees, alternate seeds) to quantify variance in ğ‘ and Î¸.
- Decide whether 3PL merits extension to PCA/MNIST or documenting as MobileNet-only.
- Finish item-tier overlays (high/medium/low ğ‘) and align them with the qualitative grids.

---

# References

- Wilson, M. (2005). <em>Constructing Measures: An Item Response Modeling Approach</em>. Lawrence Erlbaum Associates.
- Breiman, L., Friedman, J. H., Olshen, R. A., &amp; Stone, C. J. (1984). <em>Classification and Regression Trees</em>. Wadsworth.
- Breiman, L. (2001). "Random Forests." <em>Machine Learning</em>, 45(1), 5â€“32.

---

# Extra Material

--- 

# Decision Trees â€” From Data to Splits

<div class="columns">
  <div class="col">

**Idea:** recursively split data to  increase *purity* of labels (Breiman et al., 1984).  

Example:  
> â€œPetalLength < 2.5?â€ â†’ all *Setosa* left, others right.

At each node:
- compute **impurity** (e.g., *entropy* or *Gini*):
  $$ H = -\sum_i p_i \log_2 p_i $$
- choose the split that **maximally reduces impurity** â€” i.e. makes groups more uniform.  

A single tree = a set of *ifâ€“then* rules that classify or predict.
  </div>

  <div class="col">

<div class="col">

<center>

| PetalLength | PetalWidth | Species |
|--------------|-------------|----------|
| 1.4 | 0.2 | Setosa |
| 4.7 | 1.4 | Versicolor |
| 5.5 | 2.0 | Virginica |

<br />
<br />
<img width="65%" src="iris.svg">

</center>
</div>

</div>


---

# Gini vs. Entropy â€” Two Lenses on Node Impurity

<div class="columns">
<div class="col">

**Entropy (Information Theory):**

$$ H = - \sum_i p_i \log_2 p_i $$

Measures **uncertainty** â€”  expected information (in bits) needed to classify a random sample.  *High when classes are evenly mixed.*
</div>

<div class="col">

**Gini Impurity (Probability of Misclassification):**

$$ G = 1 - \sum_i p_i^2 $$

Measures **chance of error** â€”  probability that two randomly drawn samples from the node  belong to different classes.
</div>
</div>


<center>

| Metric | Theoretical Lens | Interpretation | Typical Use |
|---------|------------------|----------------|--------------|
| **Entropy** | Information theory | â€œHow surprised would I be?â€ | ID3, C4.5 trees |
| **Gini** | Probability theory | â€œHow often would I be wrong?â€ | CART trees, scikit-learn default |
</center>

> Both peak when classes are perfectly mixed (p = 0.5).  
> Gini is slightly flatter â€” faster to compute, less sensitive to extremes.


---

# Estimating Î¸ and Î´ â€” A Toy Rasch Example Worked-Out
<div class="columns">
<div class="col">

Weâ€™ll fit a **1-Parameter Logistic Model (Rasch Model)** to a tiny response matrix:

<center>

| Person | Item 1 | Item 2 | Item 3 | Total ráµ¢ |
|:------:|:------:|:------:|:------:|:---------:|
| **A** | 1 | 1 | 0 | 2 |
| **B** | 1 | 0 | 0 | 1 |
| **C** | 0 | 0 | 1 | 1 |

</center>

â€œ1â€ = correctâ€ƒâ€œ0â€ = incorrect  
Three people, three items â€” small enough to solve by hand.
</div>

<div class="col">

**The Rasch Model**:
For each person *i* and item *j*:

$$ P(X_{ij}=1|\theta_i,\delta_j)= \frac{1}{1+\exp[-(\theta_i-\delta_j)]} $$

- $(\theta_i)$: personâ€™s latent **ability**  
- $(\delta_j)$: itemâ€™s **difficulty**  

When $(\theta_i = \delta_j)$, $(P(X_{ij}=1)=0.5)$ â€” equal odds of success and failure.

</div>
</div>


---

# Step 1 â€” Initial Estimates

<div class="columns">
<div class="col">

A simple starting point uses **logits of proportions**:

$$
\theta_i^{(0)} = \log\!\frac{r_i/m}{1-r_i/m}, \qquad
\delta_j^{(0)} = -\log\!\frac{s_j/n}{1-s_j/n}
$$

where  
- $( m=3 )$ items per person  
- $( n=3 )$ people per item  
- $( r_i )$ : person totalsâ€ƒ$( s_j )$: item totals  

</div>

<div class="col">

Compute:

<center>

<div class="col">

|        | Formula | Value |
|:-------|:---------|:------|
| Resp_A | logit(2/3) |  +0.693 |
| Resp_B | logit(1/3) |  â€“0.693 |
| Resp_C | logit(1/3) |  â€“0.693 |

</div>

<div class="col">


|        | Formula | Value |
|:-------|:---------|:------|
| Item 1 | â€“logit(2/3) |  â€“0.693 |
| Item 2 | â€“logit(1/3) |  +0.693 |
| Item 3 | â€“logit(1/3) |  +0.693 |

</div>

</center>

Center the item difficulties so âˆ‘Î´ = 0.
</div>
</div>

---

# Step 2 â€” Compute Expected Scores

- Plug into the Rasch model:

$$
P_{ij} = \frac{1}{1+e^{-(\theta_i-\delta_j)}}
$$

- Sum across items â†’ expected totals per person.

<center>

| Person | Expected ráµ¢ | Observed ráµ¢ |
|:------:|:-------------:|:-------------:|
| A | 1.95 | 2 |
| B | 1.04 | 1 |
| C | 1.04 | 1 |

</center>

- Pretty close already â€” the model almost reproduces the data.

---

# Step 3 â€” One Newton Update (By Hand)

- Adjust each parameter so model-predicted totals match observed totals:

$$
\theta_i \leftarrow \theta_i +
\frac{r_i-\sum_j P_{ij}}
     {\sum_j P_{ij}(1-P_{ij})}
$$

$$
\delta_j \leftarrow \delta_j -
\frac{s_j-\sum_i P_{ij}}
     {\sum_i P_{ij}(1-P_{ij})}
$$

- Re-center âˆ‘Î´ = 0 after each step.

- After one iteration (rounded):

<center>

| Parameter | Value |
|------------|-------|
| Î¸â‚ |  +0.77 |
| Î¸áµ¦ |  â€“0.75 |
| Î¸ğ‘ |  â€“0.75 |
| Î´â‚ |  â€“1.04 |
| Î´â‚‚ |  +0.52 |
| Î´â‚ƒ |  +0.52 |

</center>

---

# Step 4 â€” Check Fit

- Recompute expectations:

<center>

| Person | Expected ráµ¢ | Observed ráµ¢ |
|:------:|:-------------:|:-------------:|
| A | 1.99 | 2 |
| B | 1.01 | 1 |
| C | 1.01 | 1 |

</center>

- Perfect alignment â€” the model now fits.

---

# What We Learned

- \( r_i \) (person totals) are **sufficient** for estimating Î¸.  
- \( s_j \) (item totals) are **sufficient** for estimating Î´.  
- Centering fixes the arbitrary origin of the latent scale.  
- Precision varies by level: extreme scores â†’ higher uncertainty.

> In the Rasch world, **abilities and difficulties calibrate each other** â€”  
> each Î¸ and Î´ defined only in relation to the ensemble.


---

# From Rasch Estimation to Stochastic Variational Inference (SVI)

Both Rasch estimation and SVI **fit latent-variable models**, but they differ in *how* they estimate hidden parameters.

| Aspect | Rasch (by hand) | Stochastic Variational Inference |
|:-------|:----------------|:--------------------------------|
| **Goal** | Find point estimates of Î¸ and Î´ that best reproduce observed responses. | Approximate full posterior distributions over latent variables. |
| **Computation** | Deterministic updates (e.g., Newtonâ€“Raphson). | Stochastic gradient ascent on an *evidence lower bound* (ELBO). |
| **Uncertainty** | Single best estimate per parameter. | Explicitly models uncertainty via variational distributions \( q(\theta,\delta) \). |
| **Scale** | Works for small datasets, exact likelihood. | Scales to millions of observations using minibatches. |
| **Analogy** | Matching observed vs. expected totals until equilibrium. | Minimizing the KL divergence between approximate and true posteriors. |

> Rasch estimation is like hand-tuning a few dials until predicted counts match reality.  
> SVI automates that process with noisy gradients â€” learning not just *where the dials land*, but *how uncertain we are about their true positions.*


